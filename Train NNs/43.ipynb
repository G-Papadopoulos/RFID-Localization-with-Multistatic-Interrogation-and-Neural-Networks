{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"p9iTin1Hf3AY"},"outputs":[],"source":["# Mount Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Import Libraries\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow_probability as tfp\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Activation\n","from tensorflow.keras.optimizers import RMSprop, Adam, SGD\n","from tensorflow.keras.initializers import glorot_normal, HeNormal\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n","from tensorflow.keras.utils import set_random_seed\n","\n","# For reproducible results set random seed\n","set_random_seed(43)\n","tf.keras.backend.set_floatx('float32')"]},{"cell_type":"markdown","metadata":{"id":"fvrLaXztiPP6"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hpl7AxNRgJ9E"},"outputs":[],"source":["# Create Custom Loss Functions\n","def custom_mean_absolute_error(y_true, y_pred):\n","    return tf.reduce_mean(tf.norm(y_true-y_pred, axis=-1))\n","\n","def custom_median_absolute_error(y_true, y_pred):\n","    return tfp.stats.percentile(tf.norm(y_true-y_pred, axis=-1), 50.0, interpolation='midpoint')\n","\n","def custom_mean_squared_error(y_true, y_pred):\n","  return tf.reduce_mean(tf.norm(y_true-y_pred, axis=-1)**2)\n","\n","def custom_median_squared_error(y_true, y_pred):\n","  return tfp.stats.percentile(tf.norm(y_true-y_pred, axis=-1)**2, 50.0, interpolation='midpoint')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t9lDOAmtgUnF"},"outputs":[],"source":["# Define the model\n","model = Sequential()\n","\n","# Input layer\n","model.add(Dense(12, input_dim=2, activation='relu'))\n","\n","# Hidden layers\n","model.add(Dense(24, activation='relu'))\n","model.add(Dense(48, activation='relu'))\n","model.add(Dense(64, activation='relu'))\n","model.add(Dense(128, activation='relu'))\n","model.add(Dense(256, activation='relu'))\n","\n","# Output layer with 2 neurons for x and y coordinates\n","model.add(Dense(2))\n","\n","model.compile(optimizer = Adam(learning_rate=10**-4),\n","              loss = custom_mean_absolute_error,\n","              metrics = ['accuracy'])\n","\n","# Import train/val data\n","data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/The chosen ones/rand_data_v4.csv')\n","\n","# Separate the input features (X) and output labels (y)\n","X = data.iloc[:10**6, 4:].values\n","Y = data.iloc[:10**6, :2].values\n","\n","# Split the data into training and testing sets\n","# Constant random state is needed in order to have the same split each time\n","X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.15)\n","\n","# Prepare the data for the NN, using Scaling\n","scaler = RobustScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_val_scaled = scaler.transform(X_val)\n","\n","# Train the model\n","history = model.fit(X_train_scaled, Y_train, epochs=50, batch_size=64, validation_data=(X_val_scaled, Y_val), shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R6DptnWxhE2T"},"outputs":[],"source":["# Visualize the results\n","import matplotlib.pyplot as plt\n","train_loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","# Create x-axis values (epochs)\n","epochs = range(1, len(train_loss) + 1)\n","\n","# Plot training & validation loss\n","plt.plot(epochs, train_loss, 'b', label='Training Loss')\n","plt.plot(epochs, val_loss, 'g', label='Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss (m)')\n","plt.ylim([0, 20])\n","\n","plt.legend()\n","\n","# Display the plot\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AHLPpoMihRuY"},"outputs":[],"source":["# Save the entire model to a HDF5 file.C:\\Users\\antik\\My Drive\\Colab Notebooks\\The chosen ones\\Working Projects\\No Amb\n","# The '.h5' extension indicates that the model should be saved to HDF5.\n","model.save('/content/drive/MyDrive/Colab Notebooks/The chosen ones/Working Projects/No Amb/Non-Optimal/relu_Dphi_mae_43.keras')\n","\n","# Save Train/Val. losses\n","train_val_loss = pd.DataFrame(\n","    {'train_loss': train_loss,\n","     'validation_loss': val_loss\n","    })\n","train_val_loss.to_csv('/content/drive/MyDrive/Colab Notebooks/The chosen ones/Working Projects/No Amb/Non-Optimal/relu_Dphi_mae_losses43.csv')"]},{"cell_type":"markdown","metadata":{"id":"X_lfp1WeiWIN"},"source":["# Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wqS7-uuViaQ1"},"outputs":[],"source":["# Rand. Tag Position\n","# For .csv files with more than a million samples\n","# Separate the input features (X) and output labels (y)\n","X_test = data.iloc[10**6+1:, 4:].values\n","Y_test = data.iloc[10**6+1:, :2].values\n","X_test_scaled = scaler.transform(X_test)\n","\n","loss_rand = model.evaluate(X_test_scaled, Y_test)\n","print(f'MAE  Test loss: {loss_rand}')\n","print(f'RMSE Test loss: {tf.sqrt(loss_rand)}')\n","\n","predictions_rand = model.predict(X_test_scaled)\n","predictions_df = pd.DataFrame(predictions_rand)\n","\n","# save the dataframe as a csv file\n","predictions_df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/The chosen ones/Predictions/No Amb/Non-Optimal/relu_Dphi_mae_predictions_rand_43.csv\")\n","\n","# Calculate all errors\n","error = custom_mean_absolute_error(Y_test, predictions_rand)\n","print(f'Mean Absolute Error: {error}')\n","\n","error = custom_median_absolute_error(Y_test, predictions_rand)\n","print(f'Median Absolute loss: {error}')\n","\n","error = custom_mean_squared_error(Y_test, predictions_rand)\n","print(f'Root Mean Squared Error: {tf.sqrt(error)}')\n","\n","error = custom_median_squared_error(Y_test, predictions_rand)\n","print(f'Root Median Squared Error: {tf.sqrt(error)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_jHqwhK8i-b3"},"outputs":[],"source":["# Scenario 1\n","test_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/The chosen ones/grid_data_scenario1_v4.csv')\n","\n","# Separate the input features (X) and output labels (y)\n","X_test = test_data.iloc[:, 4:].values\n","Y_test = test_data.iloc[:, :2].values\n","X_test_scaled = scaler.transform(X_test)\n","\n","loss_scenario1 = model.evaluate(X_test_scaled, Y_test)\n","print(f'MAE  Test loss: {loss_scenario1}')\n","print(f'RMSE Test loss: {tf.sqrt(loss_scenario1)}')\n","\n","predictions_scenario1 = model.predict(X_test_scaled)\n","predictions_df = pd.DataFrame(predictions_scenario1)\n","\n","# save the dataframe as a csv file\n","predictions_df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/The chosen ones/Predictions/No Amb/Non-Optimal/relu_Dphi_mae_predictions_scenario1_43.csv\")\n","\n","# Calculate all errors\n","error = custom_mean_absolute_error(Y_test, predictions_scenario1)\n","print(f'Mean Absolute Error: {error}')\n","\n","error = custom_median_absolute_error(Y_test, predictions_scenario1)\n","print(f'Median Absolute loss: {error}')\n","\n","error = custom_mean_squared_error(Y_test, predictions_scenario1)\n","print(f'Root Mean Squared Error: {tf.sqrt(error)}')\n","\n","error = custom_median_squared_error(Y_test, predictions_scenario1)\n","print(f'Root Median Squared Error: {tf.sqrt(error)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"psR21Pi5jEM8"},"outputs":[],"source":["# Scenario 2\n","test_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/The chosen ones/grid_data_scenario2_v4.csv')\n","\n","# Separate the input features (X) and output labels (y)\n","X_test = test_data.iloc[:, 4:].values\n","Y_test = test_data.iloc[:, :2].values\n","X_test_scaled = scaler.transform(X_test)\n","\n","loss_scenario2 = model.evaluate(X_test_scaled, Y_test)\n","print(f'MAE  Test loss: {loss_scenario2}')\n","print(f'RMSE Test loss: {tf.sqrt(loss_scenario2)}')\n","\n","predictions_scenario2 = model.predict(X_test_scaled)\n","predictions_df = pd.DataFrame(predictions_scenario2)\n","\n","# save the dataframe as a csv file\n","predictions_df.to_csv(\"/content/drive/MyDrive/Colab Notebooks/The chosen ones/Predictions/No Amb/Non-Optimal/relu_Dphi_mae_predictions_scenario2_43.csv\")\n","\n","# Calculate all errors\n","error = custom_mean_absolute_error(Y_test, predictions_scenario2)\n","print(f'Mean Absolute Error: {error}')\n","\n","error = custom_median_absolute_error(Y_test, predictions_scenario2)\n","print(f'Median Absolute loss: {error}')\n","\n","error = custom_mean_squared_error(Y_test, predictions_scenario2)\n","print(f'Root Mean Squared Error: {tf.sqrt(error)}')\n","\n","error = custom_median_squared_error(Y_test, predictions_scenario2)\n","print(f'Root Median Squared Error: {tf.sqrt(error)}')"]}],"metadata":{"colab":{"provenance":[{"file_id":"1FYM53PqjxlavM8vkAd8SpEnMsj_GBFW3","timestamp":1706564250880}],"authorship_tag":"ABX9TyPS9zUYcG/pouAE+6AzmCxm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}